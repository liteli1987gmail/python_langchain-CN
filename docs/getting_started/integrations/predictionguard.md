# é¢„æµ‹ä¿æŠ¤



>[é¢„æµ‹ä¿æŠ¤](https://docs.predictionguard.com/)æä¾›äº†ä¸€ç§å¿«é€Ÿç®€ä¾¿çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨æœ€å…ˆè¿›çš„å¼€æ”¾å’Œå°é—­è®¿é—®LLMsï¼Œæ— éœ€èŠ±è´¹æ•°å¤©æˆ–æ•°å‘¨æ¥ç†è§£æ‰€æœ‰å®ç°ç»†èŠ‚ï¼Œç®¡ç†å¤§é‡ä¸åŒçš„APIè§„èŒƒï¼Œå¹¶è®¾ç½®æ¨¡å‹éƒ¨ç½²çš„åŸºç¡€è®¾æ–½ã€‚





## å®‰è£…å’Œè®¾ç½®

- å®‰è£…Python SDKï¼š

```bash

pip install predictionguard

```



- è·å–é¢„æµ‹ä¿æŠ¤çš„è®¿é—®ä»¤ç‰Œï¼ˆè¯·å‚è§[è¿™é‡Œ](https://docs.predictionguard.com/)ï¼‰å¹¶å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼ˆ`PREDICTIONGUARD_TOKEN`ï¼‰



## LLM



```python

from langchain.llms import PredictionGuard

```



### ç¤ºä¾‹

å½“åˆå§‹åŒ–LLMæ—¶ï¼Œæ‚¨å¯ä»¥å°†Prediction Guardæ¨¡å‹çš„åç§°ä½œä¸ºå‚æ•°æä¾›ï¼š

```python

pgllm = PredictionGuard(model="MPT-7B-Instruct")

```



æ‚¨è¿˜å¯ä»¥ç›´æ¥æä¾›è®¿é—®ä»¤ç‰Œä½œä¸ºå‚æ•°ï¼š

```python

pgllm = PredictionGuard(model="MPT-7B-Instruct", token="<your access token>")

```



æ­¤å¤–ï¼Œæ‚¨å¯ä»¥æä¾›ä¸€ä¸ªâ€œoutputâ€å‚æ•°ï¼Œç”¨äºç»“æ„åŒ–/æ§åˆ¶LLMçš„è¾“å‡ºï¼š

```python

pgllm = PredictionGuard(model="MPT-7B-Instruct", output={"type": "boolean"})

```



#### æ§åˆ¶æˆ–å—ä¿æŠ¤çš„LLMçš„åŸºæœ¬ç”¨æ³•ï¼š

```python

import os



import predictionguard as pg

from langchain.llms import PredictionGuard

from langchain import PromptTemplate, LLMChain



# Your Prediction Guard API key. Get one at predictionguard.com

os.environ["PREDICTIONGUARD_TOKEN"] = "<your Prediction Guard access token>"



# Define a prompt template

template = """Respond to the following query based on the context.



Context: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! ğŸ‰ We have officially added TWO new candle subscription box options! ğŸ“¦

Exclusive Candle Box - $80 

Monthly Candle Box - $45 (NEW!)

Scent of The Month Box - $28 (NEW!)

Head to stories to get ALLL the deets on each box! ğŸ‘† BONUS: Save 50% on your first box with code 50OFF! ğŸ‰



Query: {query}



Result: """

prompt = PromptTemplate(template=template, input_variables=["query"])



# With "guarding" or controlling the output of the LLM. See the 

# Prediction Guard docs (https://docs.predictionguard.com) to learn how to 

# control the output with integer, float, boolean, JSON, and other types and

# structures.

pgllm = PredictionGuard(model="MPT-7B-Instruct", 

                        output={

                                "type": "categorical",

                                "categories": [

                                    "product announcement", 

                                    "apology", 

                                    "relational"

                                    ]

                                })

pgllm(prompt.format(query="What kind of post is this?"))

```



#### Prediction Guardçš„åŸºæœ¬LLMé“¾æ¥ï¼š

```python

import os



from langchain import PromptTemplate, LLMChain

from langchain.llms import PredictionGuard



# Optional, add your OpenAI API Key. This is optional, as Prediction Guard allows

# you to access all the latest open access models (see https://docs.predictionguard.com)

os.environ["OPENAI_API_KEY"] = "<your OpenAI api key>"



# Your Prediction Guard API key. Get one at predictionguard.com

os.environ["PREDICTIONGUARD_TOKEN"] = "<your Prediction Guard access token>"



pgllm = PredictionGuard(model="OpenAI-text-davinci-003")



template = """Question: {question}



Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])

llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)



question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"



llm_chain.predict(question=question)

```
