{"cells": [{"cell_type": "markdown", "id": "ff4be5f3", "metadata": {}, "source": ["# ConversationTokenBufferMemory\n", "\n", "`ConversationTokenBufferMemory` \u4fdd\u7559\u6700\u8fd1\u7684\u4ea4\u4e92\u8bb0\u5f55\u5728\u5185\u5b58\u4e2d\uff0c\u5e76\u4f7f\u7528 token \u957f\u5ea6\u800c\u4e0d\u662f\u4ea4\u4e92\u6b21\u6570\u6765\u786e\u5b9a\u4f55\u65f6\u5237\u65b0\u4ea4\u4e92\u3002\n", "\n", "\u9996\u5148\u8ba9\u6211\u4eec\u6765\u4e86\u89e3\u5982\u4f55\u4f7f\u7528\u5b9e\u7528\u7a0b\u5e8f\u3002"]}, {"cell_type": "code", "execution_count": 1, "id": "da3384db", "metadata": {}, "outputs": [], "source": ["from langchain.memory import ConversationTokenBufferMemory\n", "from langchain.llms import OpenAI\n", "llm = OpenAI()"]}, {"cell_type": "code", "execution_count": 2, "id": "e00d4938", "metadata": {}, "outputs": [], "source": ["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n", "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n", "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})"]}, {"cell_type": "code", "execution_count": 3, "id": "2fe28a28", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'history': 'Human: not much you\\nAI: not much'}"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["memory.load_memory_variables({})"]}, {"cell_type": "markdown", "id": "cf57b97a", "metadata": {}, "source": ["\u6211\u4eec\u4e5f\u53ef\u4ee5\u5c06\u5386\u53f2\u8bb0\u5f55\u4f5c\u4e3a\u6d88\u606f\u5217\u8868\u83b7\u53d6\uff08\u5982\u679c\u60a8\u6b63\u5728\u5c06\u5176\u4e0e\u804a\u5929\u6a21\u578b\u4e00\u8d77\u4f7f\u7528\uff0c\u5219\u6b64\u529f\u80fd\u975e\u5e38\u6709\u7528\uff09\u3002"]}, {"cell_type": "code", "execution_count": 4, "id": "3422a3a8", "metadata": {}, "outputs": [], "source": ["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10, return_messages=True)\n", "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n", "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})"]}, {"cell_type": "markdown", "id": "a6d2569f", "metadata": {}, "source": ["## \u5728\u94fe\u4e2d\u4f7f\u7528\n", "\u8ba9\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u793a\u4f8b\u6765\u6f14\u793a\uff0c\u518d\u6b21\u8bbe\u7f6e `verbose=True`\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u63d0\u793a\u3002"]}, {"cell_type": "code", "execution_count": 9, "id": "ebd68c10", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n", "Prompt after formatting:\n", "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n", "\n", "Current conversation:\n", "\n", "Human: Hi, what's up?\n", "AI:\u001b[0m\n", "\n", "\u001b[1m> Finished chain.\u001b[0m\n"]}, {"data": {"text/plain": ["\" Hi there! I'm doing great, just enjoying the day. How about you?\""]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["from langchain.chains import ConversationChain\n", "conversation_with_summary = ConversationChain(\n", "    llm=llm, \n", "    # We set a very low max_token_limit for the purposes of testing.\n", "    memory=ConversationTokenBufferMemory(llm=OpenAI(), max_token_limit=60),\n", "    verbose=True\n", ")\n", "conversation_with_summary.predict(input=\"Hi, what's up?\")"]}, {"cell_type": "code", "execution_count": 10, "id": "86207a61", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n", "Prompt after formatting:\n", "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n", "\n", "Current conversation:\n", "Human: Hi, what's up?\n", "AI:  Hi there! I'm doing great, just enjoying the day. How about you?\n", "Human: Just working on writing some documentation!\n", "AI:\u001b[0m\n", "\n", "\u001b[1m> Finished chain.\u001b[0m\n"]}, {"data": {"text/plain": ["' Sounds like a productive day! What kind of documentation are you writing?'"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["conversation_with_summary.predict(input=\"Just working on writing some documentation!\")"]}, {"cell_type": "code", "execution_count": 11, "id": "76a0ab39", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n", "Prompt after formatting:\n", "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n", "\n", "Current conversation:\n", "Human: Hi, what's up?\n", "AI:  Hi there! I'm doing great, just enjoying the day. How about you?\n", "Human: Just working on writing some documentation!\n", "AI:  Sounds like a productive day! What kind of documentation are you writing?\n", "Human: For LangChain! Have you heard of it?\n", "AI:\u001b[0m\n", "\n", "\u001b[1m> Finished chain.\u001b[0m\n"]}, {"data": {"text/plain": ["\" Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you're writing about?\""]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")"]}, {"cell_type": "code", "execution_count": 12, "id": "8c669db1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n", "Prompt after formatting:\n", "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n", "\n", "Current conversation:\n", "Human: For LangChain! Have you heard of it?\n", "AI:  Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you're writing about?\n", "Human: Haha nope, although a lot of people confuse it for that\n", "AI:\u001b[0m\n", "\n", "\u001b[1m> Finished chain.\u001b[0m\n"]}, {"data": {"text/plain": ["\" Oh, I see. Is there another language learning platform you're referring to?\""]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["# We can see here that the buffer is updated\n", "conversation_with_summary.predict(input=\"Haha nope, although a lot of people confuse it for that\")"]}, {"cell_type": "code", "execution_count": null, "id": "8c09a239", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.1"}}, "nbformat": 4, "nbformat_minor": 5}