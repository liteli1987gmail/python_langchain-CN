{"cells": [{"cell_type": "markdown", "id": "480b7cf8", "metadata": {}, "source": ["\uff03 \u95ee\u9898\u56de\u7b54\n", "\n", "\u672c\u7b14\u8bb0\u6db5\u76d6\u4e86\u5982\u4f55\u8bc4\u4f30\u901a\u7528\u95ee\u9898\u56de\u7b54\u95ee\u9898\u3002\u8fd9\u662f\u4e00\u4e2a\u60c5\u51b5\uff0c\u60a8\u6709\u4e00\u4e2a\u5305\u542b\u95ee\u9898\u53ca\u5176\u5bf9\u5e94\u7684\u771f\u5b9e\u7b54\u6848\u7684\u793a\u4f8b\uff0c\u60a8\u60f3\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\u65b9\u9762\u7684\u8868\u73b0\u5982\u4f55\u3002"]}, {"cell_type": "markdown", "id": "78e3023b", "metadata": {}, "source": ["## \u8bbe\u7f6e\n", "\n", "\u51fa\u4e8e\u6f14\u793a\u76ee\u7684\uff0c\u6211\u4eec\u5c06\u4ec5\u8bc4\u4f30\u4e00\u4e2a\u4ec5\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u7b80\u5355\u95ee\u9898\u56de\u7b54\u7cfb\u7edf\u3002\u8bf7\u53c2\u9605\u5176\u4ed6\u7b14\u8bb0\u672c\uff0c\u4ee5\u4e86\u89e3\u5b83\u5982\u4f55\u5728\u6a21\u578b\u672a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u95ee\u9898\u56de\u7b54\u7684\u793a\u4f8b\u3002"]}, {"cell_type": "code", "execution_count": 1, "id": "96710d50", "metadata": {}, "outputs": [], "source": ["from langchain.prompts import PromptTemplate\n", "from langchain.chains import LLMChain\n", "from langchain.llms import OpenAI"]}, {"cell_type": "code", "execution_count": 2, "id": "e33ccf00", "metadata": {}, "outputs": [], "source": ["prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])"]}, {"cell_type": "code", "execution_count": 3, "id": "172d993a", "metadata": {}, "outputs": [], "source": ["llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n", "chain = LLMChain(llm=llm, prompt=prompt)"]}, {"cell_type": "markdown", "id": "0c584440", "metadata": {}, "source": ["## \u793a\u4f8b\n", "\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4ec5\u4f7f\u7528\u4e24\u4e2a\u7b80\u5355\u7684\u786c\u7f16\u7801\u793a\u4f8b\uff0c\u4f46\u8bf7\u53c2\u9605\u5176\u4ed6\u7b14\u8bb0\u672c\uff0c\u4e86\u89e3\u5982\u4f55\u83b7\u5f97\u548c/\u6216\u751f\u6210\u8fd9\u4e9b\u793a\u4f8b\u7684\u63d0\u793a\u3002"]}, {"cell_type": "code", "execution_count": 4, "id": "87de1d84", "metadata": {}, "outputs": [], "source": ["examples = [\n", "    {\n", "        \"question\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n", "        \"answer\": \"11\"\n", "    },\n", "    {\n", "        \"question\": 'Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"',\n", "        \"answer\": \"No\"\n", "    }\n", "]"]}, {"cell_type": "markdown", "id": "143b1155", "metadata": {}, "source": ["## \u9884\u6d4b\n", "\n", "\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5236\u4f5c\u548c\u68c0\u67e5\u8fd9\u4e9b\u95ee\u9898\u7684\u9884\u6d4b\u3002"]}, {"cell_type": "code", "execution_count": 5, "id": "c7bd809c", "metadata": {}, "outputs": [], "source": ["predictions = chain.apply(examples)"]}, {"cell_type": "code", "execution_count": 6, "id": "f06dceab", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'text': ' 11 tennis balls'},\n", " {'text': ' No, this sentence is not plausible. Joao Moutinho is a professional soccer player, not an American football player, so it is not likely that he would be catching a screen pass in the NFC championship.'}]"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["predictions"]}, {"cell_type": "markdown", "id": "45cc2f9d", "metadata": {}, "source": ["## \u8bc4\u4f30\n", "\n", "\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5982\u679c\u6211\u4eec\u4ec5\u5bf9\u7b54\u6848\u8fdb\u884c\u7cbe\u786e\u5339\u914d\uff08`11`\u548c`No`\uff09\uff0c\u5b83\u4eec\u5c06\u65e0\u6cd5\u5339\u914d\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u3002\u4f46\u662f\uff0c\u4ece\u8bed\u4e49\u4e0a\u8bb2\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u90fd\u662f\u6b63\u786e\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u6765\u8bc4\u4f30\u7b54\u6848\u3002"]}, {"cell_type": "code", "execution_count": 7, "id": "0cacc65a", "metadata": {}, "outputs": [], "source": ["from langchain.evaluation.qa import QAEvalChain"]}, {"cell_type": "code", "execution_count": 8, "id": "5aa6cd65", "metadata": {}, "outputs": [], "source": ["llm = OpenAI(temperature=0)\n", "eval_chain = QAEvalChain.from_llm(llm)\n", "graded_outputs = eval_chain.evaluate(examples, predictions, question_key=\"question\", prediction_key=\"text\")"]}, {"cell_type": "code", "execution_count": 9, "id": "63780020", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Example 0:\n", "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n", "Real Answer: 11\n", "Predicted Answer:  11 tennis balls\n", "Predicted Grade:  CORRECT\n", "\n", "Example 1:\n", "Question: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"\n", "Real Answer: No\n", "Predicted Answer:  No, this sentence is not plausible. Joao Moutinho is a professional soccer player, not an American football player, so it is not likely that he would be catching a screen pass in the NFC championship.\n", "Predicted Grade:  CORRECT\n", "\n"]}], "source": ["for i, eg in enumerate(examples):\n", "    print(f\"Example {i}:\")\n", "    print(\"Question: \" + eg['question'])\n", "    print(\"Real Answer: \" + eg['answer'])\n", "    print(\"Predicted Answer: \" + predictions[i]['text'])\n", "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n", "    print()"]}, {"cell_type": "markdown", "id": "782ae8c8", "metadata": {}, "source": ["## \u81ea\u5b9a\u4e49\u63d0\u793a\n", "\n", "\u60a8\u4e5f\u53ef\u4ee5\u81ea\u5b9a\u4e49\u4f7f\u7528\u7684\u63d0\u793a\u3002\u8fd9\u662f\u4e00\u4e2a\u7528\u5206\u6570\u4ece0\u523010\u63d0\u793a\u5b83\u7684\u793a\u4f8b\u3002\n", "\u81ea\u5b9a\u4e49\u63d0\u793a\u9700\u89813\u4e2a\u8f93\u5165\u53d8\u91cf\uff1a\u201c\u67e5\u8be2\u201d\u3001\u201c\u7b54\u6848\u201d\u548c\u201c\u7ed3\u679c\u201d\u3002\u5176\u4e2d\uff0c\u201c\u67e5\u8be2\u201d\u662f\u95ee\u9898\uff0c\u201c\u7b54\u6848\u201d\u662f\u771f\u5b9e\u7b54\u6848\uff0c\u201c\u7ed3\u679c\u201d\u662f\u9884\u6d4b\u7b54\u6848\u3002"]}, {"cell_type": "code", "execution_count": null, "id": "153425c4", "metadata": {}, "outputs": [], "source": ["from langchain.prompts.prompt import PromptTemplate\n", "\n", "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n", "You are grading the following question:\n", "{query}\n", "Here is the real answer:\n", "{answer}\n", "You are grading the following predicted answer:\n", "{result}\n", "What grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 10 is the highest (very high similarity)?\n", "\"\"\"\n", "\n", "PROMPT = PromptTemplate(input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)"]}, {"cell_type": "code", "execution_count": null, "id": "0a3b0fb7", "metadata": {}, "outputs": [], "source": ["evalchain = QAEvalChain.from_llm(llm=llm,prompt=PROMPT)\n", "evalchain.evaluate(examples, predictions, question_key=\"question\", answer_key=\"answer\", prediction_key=\"text\")"]}, {"cell_type": "markdown", "id": "cb1cf335", "metadata": {}, "source": ["## \u65e0\u57fa\u7840\u771f\u76f8\u7684\u8bc4\u4f30\n", "\u53ef\u4ee5\u5728\u6ca1\u6709\u57fa\u7840\u771f\u76f8\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u95ee\u9898\u56de\u7b54\u7cfb\u7edf\u3002\u60a8\u9700\u8981\u4e00\u4e2a\u53cd\u6620LLM\u7528\u4e8e\u56de\u7b54\u95ee\u9898\u7684\u4fe1\u606f\u7684\"context\"\u8f93\u5165\u3002\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u53ef\u4ee5\u901a\u8fc7\u4efb\u4f55\u68c0\u7d22\u7cfb\u7edf\u83b7\u5f97\u3002\u4ee5\u4e0b\u662f\u5b83\u7684\u5de5\u4f5c\u539f\u7406\u7684\u4f8b\u5b50\uff1a"]}, {"cell_type": "code", "execution_count": null, "id": "6c59293f", "metadata": {}, "outputs": [], "source": ["context_examples = [\n", "    {\n", "        \"question\": \"How old am I?\",\n", "        \"context\": \"I am 30 years old. I live in New York and take the train to work everyday.\",\n", "    },\n", "    {\n", "        \"question\": 'Who won the NFC championship game in 2023?\"',\n", "        \"context\": \"NFC Championship Game 2023: Philadelphia Eagles 31, San Francisco 49ers 7\"\n", "    }\n", "]\n", "QA_PROMPT = \"Answer the question based on the  context\\nContext:{context}\\nQuestion:{question}\\nAnswer:\"\n", "template = PromptTemplate(input_variables=[\"context\", \"question\"], template=QA_PROMPT)\n", "qa_chain = LLMChain(llm=llm, prompt=template)\n", "predictions = qa_chain.apply(context_examples)"]}, {"cell_type": "code", "execution_count": 12, "id": "e500d0cc", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'text': 'You are 30 years old.'},\n", " {'text': ' The Philadelphia Eagles won the NFC championship game in 2023.'}]"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["predictions"]}, {"cell_type": "code", "execution_count": 9, "id": "6d8cbc1d", "metadata": {}, "outputs": [], "source": ["from langchain.evaluation.qa import ContextQAEvalChain\n", "eval_chain = ContextQAEvalChain.from_llm(llm)\n", "graded_outputs = eval_chain.evaluate(context_examples, predictions, question_key=\"question\", prediction_key=\"text\")"]}, {"cell_type": "code", "execution_count": 13, "id": "6c5262d0", "metadata": {}, "outputs": [{"data": {"text/plain": ["[{'text': ' CORRECT'}, {'text': ' CORRECT'}]"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["graded_outputs"]}, {"cell_type": "markdown", "id": "aaa61f0c", "metadata": {}, "source": ["## \u4e0e\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u6bd4\u8f83\n", "\u6211\u4eec\u53ef\u4ee5\u5c06\u83b7\u5f97\u7684\u8bc4\u4f30\u7ed3\u679c\u4e0e\u5176\u4ed6\u5e38\u89c1\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u6bd4\u8f83\u3002\u4e3a\u6b64\uff0c\u8ba9\u6211\u4eec\u4eceHuggingFace\u7684`evaluate`\u5305\u4e2d\u52a0\u8f7d\u4e00\u4e9b\u8bc4\u4f30\u6307\u6807\u3002"]}, {"cell_type": "code", "execution_count": 10, "id": "d851453b", "metadata": {}, "outputs": [], "source": ["# Some data munging to get the examples in the right format\n", "for i, eg in enumerate(examples):\n", "    eg['id'] = str(i)\n", "    eg['answers'] = {\"text\": [eg['answer']], \"answer_start\": [0]}\n", "    predictions[i]['id'] = str(i)\n", "    predictions[i]['prediction_text'] = predictions[i]['text']\n", "\n", "for p in predictions:\n", "    del p['text']\n", "\n", "new_examples = examples.copy()\n", "for eg in new_examples:\n", "    del eg ['question']\n", "    del eg['answer']"]}, {"cell_type": "code", "execution_count": 11, "id": "c38eb3e9", "metadata": {"scrolled": true}, "outputs": [], "source": ["from evaluate import load\n", "squad_metric = load(\"squad\")\n", "results = squad_metric.compute(\n", "    references=new_examples,\n", "    predictions=predictions,\n", ")"]}, {"cell_type": "code", "execution_count": 12, "id": "07d68f85", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'exact_match': 0.0, 'f1': 28.125}"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["results"]}, {"cell_type": "code", "execution_count": null, "id": "3b775150", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16"}, "vscode": {"interpreter": {"hash": "53f3bc57609c7a84333bb558594977aa5b4026b1d6070b93987956689e367341"}}}, "nbformat": 4, "nbformat_minor": 5}