# LangChain Decorators âœ¨LangChainè£…é¥°å™¨âœ¨


lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar ğŸ­ for writing custom langchain prompts and chainsLangChainè£…é¥°å™¨æ˜¯LangChainçš„ä¸€å±‚ï¼Œä¸ºç¼–å†™è‡ªå®šä¹‰LangChainæç¤ºå’Œé“¾æä¾›è¯­æ³•ç³–ğŸ­


For Feedback, Issues, Contributions - please raise an issue here: æœ‰åé¦ˆ,é—®é¢˜,è´¡çŒ®-è¯·åœ¨æ­¤æå‡ºé—®é¢˜:
[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)






Main principles and benefits:



- more `pythonic` way of writing codeæ›´ç¬¦åˆPythoné£æ ¼çš„ä»£ç ç¼–å†™æ–¹å¼
- write multiline prompts that wont break your code flow with indentationç¼–å†™å¤šè¡Œæç¤ºï¼Œä¸ä¼šå› ç¼©è¿›è€Œä¸­æ–­ä»£ç æµ
- making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc.åˆ©ç”¨IDEå†…ç½®çš„æ”¯æŒå®ç°**æç¤º**, **ç±»å‹æ£€æŸ¥**å’Œ**å¼¹å‡ºæ–‡æ¡£**ï¼Œä»¥å¿«é€ŸæŸ¥çœ‹å‡½æ•°ä¸­çš„æç¤º,å‚æ•°ç­‰
- leverage all the power of ğŸ¦œğŸ”— LangChain ecosystemåˆ©ç”¨ğŸ¦œğŸ”— LangChainç”Ÿæ€ç³»ç»Ÿçš„æ‰€æœ‰åŠŸèƒ½
- adding support for **optional parameters**æ·»åŠ å¯¹**å¯é€‰å‚æ•°**çš„æ”¯æŒ
- easily share parameters between the prompts by binding them to one classé€šè¿‡å°†å‚æ•°ç»‘å®šåˆ°ä¸€ä¸ªç±»å®ç°åœ¨æç¤ºä¹‹é—´è½»æ¾å…±äº«å‚æ•°






Here is a simple example of a code written with **LangChain Decorators âœ¨**è¿™æ˜¯ä½¿ç”¨**LangChainè£…é¥°å™¨âœ¨**ç¼–å†™çš„ç®€å•ä»£ç ç¤ºä¾‹


``` python


:llm_prompt

def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:

    """

    Write me a short header for my post about {topic} for {platform} platform. 

    It should be for {audience} audience.

    (Max 15 words)

    """

    return



# run it naturaly

write_me_short_post(topic="starwars")

# or

write_me_short_post(topic="starwars", platform="redit")

```



# Quick startå¿«é€Ÿå…¥é—¨
## Installationå®‰è£…
```bash
pip install langchain_decorators

```



## Examplesç¤ºä¾‹


Good idea on how to start is to review the examples here:å¼€å§‹çš„å¥½æ–¹æ³•æ˜¯æŸ¥çœ‹è¿™é‡Œçš„ç¤ºä¾‹:
 - [jupyter notebook](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)

 - [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)



# Defining other parameterså®šä¹‰å…¶ä»–å‚æ•°
Here we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running it åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªæ˜¯ç”¨`llm_prompt`è£…é¥°å™¨å°†å‡½æ•°æ ‡è®°ä¸ºæç¤ºï¼Œå®é™…ä¸Šå°†å…¶è½¬æ¢ä¸ºLLMChainã€‚è€Œä¸æ˜¯è¿è¡Œå®ƒ




Standard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator.æ ‡å‡†LLMchainæ‰€éœ€çš„åˆå§‹åŒ–å‚æ•°è¿œè¿œä¸æ­¢inputs_variableså’Œpromptâ€¦è¿™ä¸ªå®ç°ç»†èŠ‚éšè—åœ¨è£…é¥°å™¨ä¸­ã€‚
Here is how it works:å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„


1. Using **Global settings**:1. ä½¿ç”¨**å…¨å±€è®¾ç½®**


``` python
# define global settings for all prompty (if not set - chatGPT is the current default)

from langchain_decorators import GlobalSettings



GlobalSettings.define_settings(

    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally

    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming

)

```



2. Using predefined **prompt types**



``` python
#You can change the default prompt types

from langchain_decorators import PromptTypes, PromptTypeSettings



PromptTypes.AGENT_REASONING.llm = ChatOpenAI()



# Or you can just define your own ones:

class MyCustomPromptTypes(PromptTypes):

    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))



:llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) 

def write_a_complicated_code(app_idea:str)->str:

    ...



```


3. ç›´æ¥åœ¨è£…é¥°å™¨ä¸­å®šä¹‰è®¾ç½® **

``` python
from langchain.llms import OpenAI



:llm_prompt(

    llm=OpenAI(temperature=0.7),

    stop_tokens=["Observation"],

    ...

    )

def creative_writer(book_title:str)->str:

    ...

```


## ä¼ é€’å†…å­˜å’Œ/æˆ–å›è°ƒå‡½æ•°:

è¦ä¼ é€’è¿™äº›ä¸œè¥¿ï¼Œåªéœ€åœ¨å‡½æ•°ä¸­å£°æ˜å®ƒä»¬ï¼ˆæˆ–ä½¿ç”¨kwargsä¼ é€’ä»»ä½•å†…å®¹ï¼‰

``` python


:llm_prompt()

async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):

    """

    {history_key}

    Write me a short header for my post about {topic} for {platform} platform. 

    It should be for {audience} audience.

    (Max 15 words)

    """

    pass



await write_me_short_post(topic="old movies")



```


# ç®€åŒ–æµå¼ä¼ è¾“

å¦‚æœæˆ‘ä»¬æƒ³åˆ©ç”¨æµå¼ä¼ è¾“ :
 - we need to define prompt as async function 

 - turn on the streaming on the decorator, or we can define PromptType with streaming on

 - capture the stream using StreamingContext


è¿™æ ·ï¼Œæˆ‘ä»¬åªéœ€æ ‡è®°å“ªä¸ªæç¤ºåº”è¯¥è¢«æµï¼Œè€Œä¸éœ€è¦è°ƒæ•´ä½¿ç”¨å“ªä¸ªLLMï¼Œåœ¨æˆ‘ä»¬çš„é“¾çš„ç‰¹å®šéƒ¨åˆ†ä¼ é€’å¹¶åˆ†å‘æµå¼å¤„ç†ç¨‹åº...

åªæœ‰åœ¨æµä¸Šä¸‹æ–‡ä¸­è°ƒç”¨æ—¶ï¼Œæµå¼ä¼ è¾“æ‰ä¼šå‘ç”Ÿ...åœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥å¤„ç†æµ

``` python
# this code example is complete and should run as it is



from langchain_decorators import StreamingContext, llm_prompt



# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)

# note that only async functions can be streamed (will get an error if it's not)

:llm_prompt(capture_stream=True) 

async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):

    """

    Write me a short header for my post about {topic} for {platform} platform. 

    It should be for {audience} audience.

    (Max 15 words)

    """

    pass







# just an arbitrary  function to demonstrate the streaming... wil be some websockets code in the real world

tokens=[]

def capture_stream_func(new_token:str):

    tokens.append(new_token)



# if we want to capture the stream, we need to wrap the execution into StreamingContext... 

# this will allow us to capture the stream even if the prompt call is hidden inside higher level method

# only the prompts marked with capture_stream will be captured here

with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):

    result = await run_prompt()

    print("Stream finished ... we can distinguish tokens thanks to alternating colors")





print("We've captured",len(tokens),"tokensğŸ‰")

print("Here is the result:")

print(result)

```



# æç¤ºå£°æ˜
é»˜è®¤æƒ…å†µä¸‹ï¼Œæç¤ºæ˜¯æ•´ä¸ªå‡½æ•°æ–‡æ¡£ï¼Œé™¤éæ‚¨æ ‡è®°äº†æç¤º

## è®°å½•æ‚¨çš„æç¤º

æˆ‘ä»¬å¯ä»¥é€šè¿‡æŒ‡å®šå¸¦æœ‰ **<prompt>** è¯­è¨€æ ‡è®°çš„ä»£ç å—æ¥æŒ‡å®šæˆ‘ä»¬çš„æ–‡æ¡£çš„å“ªä¸€éƒ¨åˆ†æ˜¯æç¤ºå®šä¹‰

``` python
:llm_prompt

def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):

    """

    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.



    It needs to be a code block, marked as a `<prompt>` language

    ```<prompt>

    Write me a short header for my post about {topic} for {platform} platform. 

    It should be for {audience} audience.

    (Max 15 words)

    ```



    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.

    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))

    """

    return 

```


## èŠå¤©æ¶ˆæ¯æç¤º

å¯¹äºèŠå¤©æ¨¡å‹è€Œè¨€ï¼Œå°†æç¤ºå®šä¹‰ä¸ºä¸€ç»„æ¶ˆæ¯æ¨¡æ¿éå¸¸æœ‰ç”¨...ä»¥ä¸‹æ˜¯å¦‚ä½•æ‰§è¡Œçš„ :

``` python
:llm_prompt

def simulate_conversation(human_input:str, agent_role:str="a pirate"):

    """

    ## System message

     - note the `:system` sufix inside the <prompt:_role_> tag

     



    ```<prompt:system>

    You are a {agent_role} hacker. You mus act like one.

    You reply always in code, using python or javascript code block...

    for example:

    

    ... do not reply with anything else.. just with code - respecting your role.

    ```



    # human message 

    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)

    ``` <prompt:user>

    Helo, who are you

    ```

    a reply:

    



    ``` <prompt:assistant>

    \``` python <<- escaping inner code block with \ that should be part of the prompt

    def hello():

        print("Argh... hello you pesky pirate")

    \```

    ```

    

    we can also add some history using placeholder

    ```<prompt:placeholder>

    {history}

    ```

    ```<prompt:user>

    {human_input}

    ```



    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.

    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))

    """

    pass



```


è¿™é‡Œçš„è§’è‰²æ˜¯æ¨¡å‹çš„æœ¬æœºè§’è‰²ï¼ˆèŠå¤©GPTçš„åŠ©æ‰‹,ç”¨æˆ·,ç³»ç»Ÿï¼‰



# å¯é€‰éƒ¨åˆ†
- æ‚¨å¯ä»¥å®šä¹‰æ•´ä¸ªæç¤ºçš„å¯é€‰éƒ¨åˆ†
- å¦‚æœéƒ¨åˆ†ä¸­çš„ä»»ä½•è¾“å…¥éƒ½ä¸¢å¤±ï¼Œåˆ™æ•´ä¸ªéƒ¨åˆ†å°†ä¸ä¼šå‘ˆç°

è¿™ä¸ªè¯­æ³•çš„å†™æ³•å¦‚ä¸‹:


``` python
:llm_prompt

def prompt_with_optional_partials():

    """

    this text will be rendered always, but



    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}



    you can also place it in between the words

    this too will be rendered{? , but

        this  block will be rendered only if {this_value} and {this_value}

        is not empty?} !

    """

```



# è¾“å‡ºè§£æå™¨

- llm_promptä¿®é¥°å™¨ä¼šè‡ªåŠ¨æ£€æµ‹æœ€ä½³çš„è¾“å‡ºè§£æå™¨ï¼ˆå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²ï¼‰
- list/dictå’Œpydanticç±»å‹çš„è¾“å‡ºä¹Ÿä¼šè¢«è‡ªåŠ¨æ”¯æŒ

``` python
# this code example is complete and should run as it is



from langchain_decorators import llm_prompt



:llm_prompt

def write_name_suggestions(company_business:str, count:int)->list:

    """ Write me {count} good name suggestions for company that {company_business}

    """

    pass



write_name_suggestions(company_business="sells cookies", count=5)

```


## æ›´å¤æ‚çš„ç»“æ„

å¯¹äºdict/pydanticç±»å‹ï¼Œæ‚¨éœ€è¦æŒ‡å®šæ ¼å¼åŒ–æŒ‡ä»¤â€¦
è¿™å¯èƒ½å¾ˆä¹å‘³ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‚¨å¯ä»¥è®©è¾“å‡ºè§£æå™¨åŸºäºæ¨¡å‹ï¼ˆpydanticï¼‰ä¸ºæ‚¨ç”ŸæˆæŒ‡ä»¤

``` python
from langchain_decorators import llm_prompt

from pydantic import BaseModel, Field





class TheOutputStructureWeExpect(BaseModel):

    name:str = Field (description="The name of the company")

    headline:str = Field( description="The description of the company (for landing page)")

    employees:list[str] = Field(description="5-8 fake employee names with their positions")



:llm_prompt()

def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:

    """ Generate a fake company that {company_business}

    {FORMAT_INSTRUCTIONS}

    """

    return



company = fake_company_generator(company_business="sells cookies")



# print the result nicely formatted

print("Company name: ",company.name)

print("company headline: ",company.headline)

print("company employees: ",company.employees)



```



# å°†æç¤ºç»‘å®šåˆ°å¯¹è±¡

``` python
from pydantic import BaseModel

from langchain_decorators import llm_prompt



class AssistantPersonality(BaseModel):

    assistant_name:str

    assistant_role:str

    field:str



    :property

    def a_property(self):

        return "whatever"



    def hello_world(self, function_kwarg:str=None):

        """

        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method

        """



    

    :llm_prompt

    def introduce_your_self(self)->str:

        """

        ```Â <prompt:system>

        You are an assistant named {assistant_name}. 

        Your role is to act as {assistant_role}

        ```

        ```<prompt:user>

        Introduce your self (in less than 20 words)

        ```

        """



    



personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")



print(personality.introduce_your_self(personality))

```



# æ›´å¤šç¤ºä¾‹:

- è¿™äº›å’Œå…¶ä»–å‡ ä¸ªç¤ºä¾‹ä¹Ÿå¯ä»¥åœ¨[æ­¤å¤„çš„colabç¬”è®°æœ¬](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)ä¸­æ‰¾åˆ°
- including the [ReAct Agent re-implementation](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) using purely langchain decorators

