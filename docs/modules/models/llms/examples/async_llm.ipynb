{"cells": [{"cell_type": "markdown", "id": "f6574496-b360-4ffa-9523-7fd34a590164", "metadata": {}, "source": ["# \u5982\u4f55\u4f7f\u7528 LLM \u7684\u5f02\u6b65 API\n", "\n", "LangChain \u5229\u7528 [asyncio](https://docs.python.org/3/library/asyncio.html) \u5e93\u63d0\u4f9b LLM \u7684\u5f02\u6b65\u652f\u6301\u3002\n", "\n", "\u5f02\u6b65\u652f\u6301\u5bf9\u4e8e\u540c\u65f6\u8c03\u7528\u591a\u4e2a LLM \u7279\u522b\u6709\u7528\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u8c03\u7528\u53d7\u7f51\u7edc\u9650\u5236\u3002\u76ee\u524d\u652f\u6301 `OpenAI`\u3001`PromptLayerOpenAI`\u3001`ChatOpenAI` \u548c `Anthropic`\uff0c\u4f46\u662f\u5bf9\u4e8e\u5176\u4ed6 LLM \u7684\u5f02\u6b65\u652f\u6301\u4e5f\u5728\u8def\u4e0a\u3002\n", "\n", "\u60a8\u53ef\u4ee5\u4f7f\u7528 `agenerate` \u65b9\u6cd5\u5f02\u6b65\u8c03\u7528 OpenAI LLM\u3002"]}, {"cell_type": "code", "execution_count": 1, "id": "5e49e96c-0f88-466d-b3d3-ea0966bdf19e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, how about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about yourself?\n", "\n", "\n", "I'm doing well, thank you! How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you! How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\u001b[1mConcurrent executed in 1.39 seconds.\u001b[0m\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about yourself?\n", "\n", "\n", "I'm doing well, thanks for asking. How about you?\n", "\n", "\n", "I'm doing well, thanks! How about you?\n", "\n", "\n", "I'm doing well, thank you. How about you?\n", "\n", "\n", "I'm doing well, thank you. How about yourself?\n", "\n", "\n", "I'm doing well, thanks for asking. How about you?\n", "\u001b[1mSerial executed in 5.77 seconds.\u001b[0m\n"]}], "source": ["import time\n", "import asyncio\n", "\n", "from langchain.llms import OpenAI\n", "\n", "def generate_serially():\n", "    llm = OpenAI(temperature=0.9)\n", "    for _ in range(10):\n", "        resp = llm.generate([\"Hello, how are you?\"])\n", "        print(resp.generations[0][0].text)\n", "\n", "\n", "async def async_generate(llm):\n", "    resp = await llm.agenerate([\"Hello, how are you?\"])\n", "    print(resp.generations[0][0].text)\n", "\n", "\n", "async def generate_concurrently():\n", "    llm = OpenAI(temperature=0.9)\n", "    tasks = [async_generate(llm) for _ in range(10)]\n", "    await asyncio.gather(*tasks)\n", "\n", "\n", "s = time.perf_counter()\n", "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n", "await generate_concurrently() \n", "elapsed = time.perf_counter() - s\n", "print('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n", "\n", "s = time.perf_counter()\n", "generate_serially()\n", "elapsed = time.perf_counter() - s\n", "print('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')"]}, {"cell_type": "code", "execution_count": null, "id": "e1d3a966-3a27-44e8-9441-ed72f01b86f4", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.9"}}, "nbformat": 4, "nbformat_minor": 5}