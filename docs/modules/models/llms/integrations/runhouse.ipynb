{"cells": [{"cell_type": "markdown", "id": "9597802c", "metadata": {}, "source": ["## \u8fd0\u884c\u5c4b\n", "\n", "[Runhouse](https://github.com/run-house/runhouse) \u5141\u8bb8\u7528\u6237\u8de8\u73af\u5883\u548c\u7528\u6237\u8fdb\u884c\u8fdc\u7a0b\u8ba1\u7b97\u548c\u6570\u636e\u5904\u7406\u3002\u8bf7\u53c2\u9605 [Runhouse \u6587\u6863](https://runhouse-docs.readthedocs-hosted.com/en/latest/)\u3002\n", "\n", "\u8fd9\u4e2a\u793a\u4f8b\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528 LangChain \u548c [Runhouse](https://github.com/run-house/runhouse) \u4e0e\u6258\u7ba1\u5728\u60a8\u81ea\u5df1\u7684 GPU \u4e0a\u7684\u6a21\u578b\u6216\u5728 AWS\u3001GCP\u3001AWS \u6216 Lambda \u4e0a\u4f7f\u7528\u6309\u9700 GPU \u8fdb\u884c\u4ea4\u4e92\u3002\n", "\n", "**\u6ce8\u610f**\uff1a\u4ee3\u7801\u4f7f\u7528 `SelfHosted` \u540d\u79f0\uff0c\u800c\u4e0d\u662f `Runhouse`\u3002\n"]}, {"cell_type": "code", "execution_count": null, "id": "6066fede-2300-4173-9722-6f01f4fa34b4", "metadata": {"tags": []}, "outputs": [], "source": ["!pip install runhouse"]}, {"cell_type": "code", "execution_count": 1, "id": "6fb585dd", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO | 2023-04-17 16:47:36,173 | No auth token provided, so not using RNS API to save and load configs\n"]}], "source": ["from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\n", "from langchain import PromptTemplate, LLMChain\n", "import runhouse as rh"]}, {"cell_type": "code", "execution_count": 4, "id": "06d6866e", "metadata": {"tags": []}, "outputs": [], "source": ["# For an on-demand A100 with GCP, Azure, or Lambda\n", "gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\", use_spot=False)\n", "\n", "# For an on-demand A10G with AWS (no single A100s on AWS)\n", "# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n", "\n", "# For an existing cluster\n", "# gpu = rh.cluster(ips=['<ip of the cluster>'], \n", "#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n", "#                  name='rh-a10x')"]}, {"cell_type": "code", "execution_count": 5, "id": "035dea0f", "metadata": {"tags": []}, "outputs": [], "source": ["template = \"\"\"Question: {question}\n", "\n", "Answer: Let's think step by step.\"\"\"\n", "\n", "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]}, {"cell_type": "code", "execution_count": null, "id": "3f3458d9", "metadata": {"tags": []}, "outputs": [], "source": ["llm = SelfHostedHuggingFaceLLM(model_id=\"gpt2\", hardware=gpu, model_reqs=[\"pip:./\", \"transformers\", \"torch\"])"]}, {"cell_type": "code", "execution_count": 6, "id": "a641dbd9", "metadata": {}, "outputs": [], "source": ["llm_chain = LLMChain(prompt=prompt, llm=llm)"]}, {"cell_type": "code", "execution_count": 31, "id": "6fb6fdb2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO | 2023-02-17 05:42:23,537 | Running _generate_text via gRPC\n", "INFO | 2023-02-17 05:42:24,016 | Time to send message: 0.48 seconds\n"]}, {"data": {"text/plain": ["\"\\n\\nLet's say we're talking sports teams who won the Super Bowl in the year Justin Beiber\""]}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": ["question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n", "\n", "llm_chain.run(question)"]}, {"cell_type": "markdown", "id": "c88709cd", "metadata": {}, "source": ["\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7 SelfHostedHuggingFaceLLM \u63a5\u53e3\u52a0\u8f7d\u66f4\u591a\u81ea\u5b9a\u4e49\u6a21\u578b\uff1a"]}, {"cell_type": "code", "execution_count": null, "id": "22820c5a", "metadata": {"scrolled": true}, "outputs": [], "source": ["llm = SelfHostedHuggingFaceLLM(\n", "    model_id=\"google/flan-t5-small\",\n", "    task=\"text2text-generation\",\n", "    hardware=gpu,\n", ")"]}, {"cell_type": "code", "execution_count": 39, "id": "1528e70f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO | 2023-02-17 05:54:21,681 | Running _generate_text via gRPC\n", "INFO | 2023-02-17 05:54:21,937 | Time to send message: 0.25 seconds\n"]}, {"data": {"text/plain": ["'berlin'"]}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": ["llm(\"What is the capital of Germany?\")"]}, {"cell_type": "markdown", "id": "7a0c3746", "metadata": {}, "source": ["\u4f7f\u7528\u81ea\u5b9a\u4e49\u52a0\u8f7d\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5c06\u81ea\u5b9a\u4e49 pipeline \u52a0\u8f7d\u5230\u8fdc\u7a0b\u786c\u4ef6\u4e0a\uff1a"]}, {"cell_type": "code", "execution_count": 34, "id": "893eb1d3", "metadata": {}, "outputs": [], "source": ["def load_pipeline():\n", "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # Need to be inside the fn in notebooks\n", "    model_id = \"gpt2\"\n", "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n", "    model = AutoModelForCausalLM.from_pretrained(model_id)\n", "    pipe = pipeline(\n", "        \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n", "    )\n", "    return pipe\n", "\n", "def inference_fn(pipeline, prompt, stop = None):\n", "    return pipeline(prompt)[0][\"generated_text\"][len(prompt):]"]}, {"cell_type": "code", "execution_count": null, "id": "087d50dc", "metadata": {"scrolled": true}, "outputs": [], "source": ["llm = SelfHostedHuggingFaceLLM(model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn)"]}, {"cell_type": "code", "execution_count": 36, "id": "feb8da8e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO | 2023-02-17 05:42:59,219 | Running _generate_text via gRPC\n", "INFO | 2023-02-17 05:42:59,522 | Time to send message: 0.3 seconds\n"]}, {"data": {"text/plain": ["'john w. bush'"]}, "execution_count": 36, "metadata": {}, "output_type": "execute_result"}], "source": ["llm(\"Who is the current US president?\")"]}, {"cell_type": "markdown", "id": "af08575f", "metadata": {}, "source": ["\u60a8\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u7f51\u7edc\u5c06 pipeline \u53d1\u9001\u5230\u60a8\u7684\u6a21\u578b\uff0c\u4f46\u8fd9\u53ea\u9002\u7528\u4e8e\u5c0f\u6a21\u578b\uff08<2 Gb\uff09\uff0c\u800c\u4e14\u901f\u5ea6\u5f88\u6162\uff1a"]}, {"cell_type": "code", "execution_count": null, "id": "d23023b9", "metadata": {}, "outputs": [], "source": ["pipeline = load_pipeline()\n", "llm = SelfHostedPipeline.from_pipeline(\n", "    pipeline=pipeline, hardware=gpu, model_reqs=model_reqs\n", ")"]}, {"cell_type": "markdown", "id": "fcb447a1", "metadata": {}, "source": ["\u76f8\u53cd\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u5c06\u5b83\u53d1\u9001\u5230\u786c\u4ef6\u7684\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u5c06\u66f4\u5feb\u3002"]}, {"cell_type": "code", "execution_count": null, "id": "7206b7d6", "metadata": {}, "outputs": [], "source": ["rh.blob(pickle.dumps(pipeline), path=\"models/pipeline.pkl\").save().to(gpu, path=\"models\")\n", "\n", "llm = SelfHostedPipeline.from_pipeline(pipeline=\"models/pipeline.pkl\", hardware=gpu)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.6"}}, "nbformat": 4, "nbformat_minor": 5}