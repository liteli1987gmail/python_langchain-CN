{"cells": [{"cell_type": "markdown", "id": "984169ca", "metadata": {}, "source": ["# \u95ee\u9898\u56de\u7b54\u57fa\u51c6\u6d4b\u8bd5\uff1aPaul Graham Essay\uff08\u4fdd\u7f57\u00b7\u683c\u96f7\u5384\u59c6Essay\uff09\n", "\n", "\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5bf9Paul Graham Essay\u4e0a\u7684\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u8fdb\u884c\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002\n", "\n", "\u5f3a\u70c8\u5efa\u8bae\u60a8\u5728\u542f\u7528\u8ddf\u8e2a\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4efb\u4f55\u8bc4\u4f30/\u57fa\u51c6\u6d4b\u8bd5\u3002\u6709\u5173\u8ddf\u8e2a\u662f\u4ec0\u4e48\u4ee5\u53ca\u5982\u4f55\u8bbe\u7f6e\u7684\u8bf4\u660e\uff0c\u8bf7\u53c2\u89c1[\u6b64\u5904]\uff08https://langchain.readthedocs.io/en/latest/tracing.html\uff09\u3002\n"]}, {"cell_type": "code", "execution_count": 1, "id": "3bd13ab7", "metadata": {}, "outputs": [], "source": ["# Comment this out if you are NOT using tracing\n", "import os\n", "os.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\""]}, {"cell_type": "markdown", "id": "8a16b75d", "metadata": {}, "source": ["## \u52a0\u8f7d\u6570\u636e\n", "\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u52a0\u8f7d\u6570\u636e\u3002"]}, {"cell_type": "code", "execution_count": 2, "id": "5b2d5e98", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Found cached dataset json (/Users/harrisonchase/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--question-answering-paul-graham-76e8f711e038d742/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9264acfe710b4faabf060f0fcf4f7308", "version_major": 2, "version_minor": 0}, "text/plain": ["  0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from langchain.evaluation.loading import load_dataset\n", "dataset = load_dataset(\"question-answering-paul-graham\")"]}, {"cell_type": "markdown", "id": "4ab6a716", "metadata": {}, "source": ["## \u8bbe\u7f6e\u94fe\n", "\u73b0\u5728\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e9b\u7528\u4e8e\u6267\u884c\u95ee\u9898\u56de\u7b54\u7684\u6d41\u6c34\u7ebf\u3002\u9996\u5148\u662f\u5728\u8981\u95ee\u7684\u6570\u636e\u4e0a\u521b\u5efa\u7d22\u5f15\u3002"]}, {"cell_type": "code", "execution_count": 3, "id": "c18680b5", "metadata": {}, "outputs": [], "source": ["from langchain.document_loaders import TextLoader\n", "loader = TextLoader(\"../../modules/paul_graham_essay.txt\")"]}, {"cell_type": "code", "execution_count": 4, "id": "7f0de2b3", "metadata": {}, "outputs": [], "source": ["from langchain.indexes import VectorstoreIndexCreator"]}, {"cell_type": "code", "execution_count": 5, "id": "ef84ff99", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Running Chroma using direct local API.\n", "Using DuckDB in-memory for database. Data will be transient.\n"]}], "source": ["vectorstore = VectorstoreIndexCreator().from_loaders([loader]).vectorstore"]}, {"cell_type": "markdown", "id": "f0b5d8f6", "metadata": {}, "source": ["\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u95ee\u9898\u56de\u7b54\u94fe\u3002"]}, {"cell_type": "code", "execution_count": 6, "id": "8843cb0c", "metadata": {}, "outputs": [], "source": ["from langchain.chains import RetrievalQA\n", "from langchain.llms import OpenAI"]}, {"cell_type": "code", "execution_count": 7, "id": "573719a0", "metadata": {}, "outputs": [], "source": ["chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever(), input_key=\"question\")"]}, {"cell_type": "markdown", "id": "53b5aa23", "metadata": {}, "source": ["## \u8fdb\u884c\u9884\u6d4b\n", "\n", "\u9996\u5148\uff0c\u6211\u4eec\u53ef\u4ee5\u4e00\u6b21\u9884\u6d4b\u4e00\u4e2a\u6570\u636e\u70b9\u3002\u4ee5\u8fd9\u79cd\u7c92\u5ea6\u8fd0\u884c\u53ef\u4ee5\u8ba9\u6211\u4eec\u8be6\u7ec6\u5730\u63a2\u7d22\u8f93\u51fa\uff0c\u5e76\u4e14\u6bd4\u5bf9\u591a\u4e2a\u6570\u636e\u70b9\u7684\u8fd0\u884c\u901f\u5ea6\u66f4\u5feb\u3002"]}, {"cell_type": "code", "execution_count": 18, "id": "3f81d951", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'question': 'What were the two main things the author worked on before college?',\n", " 'answer': 'The two main things the author worked on before college were writing and programming.',\n", " 'result': ' Writing and programming.'}"]}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": ["chain(dataset[0])"]}, {"cell_type": "markdown", "id": "d0c16cd7", "metadata": {}, "source": ["## \u8fdb\u884c\u591a\u6b21\u9884\u6d4b\n", "\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u591a\u6b21\u9884\u6d4b\u3002"]}, {"cell_type": "code", "execution_count": 9, "id": "24b4c66e", "metadata": {}, "outputs": [], "source": ["predictions = chain.apply(dataset)"]}, {"cell_type": "markdown", "id": "49d969fb", "metadata": {}, "source": ["## \u8bc4\u4f30\u6027\u80fd\n", "\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u8bc4\u4f30\u9884\u6d4b\u7ed3\u679c\u3002\u9996\u5148\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8089\u773c\u89c2\u5bdf\u5b83\u4eec\u3002"]}, {"cell_type": "code", "execution_count": 10, "id": "1d583f03", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'question': 'What were the two main things the author worked on before college?',\n", " 'answer': 'The two main things the author worked on before college were writing and programming.',\n", " 'result': ' Writing and programming.'}"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["predictions[0]"]}, {"cell_type": "markdown", "id": "4783344b", "metadata": {}, "source": ["\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u5bf9\u5b83\u4eec\u8fdb\u884c\u7f16\u7a0b\u8bc4\u5206\u3002"]}, {"cell_type": "code", "execution_count": 11, "id": "d0a9341d", "metadata": {}, "outputs": [], "source": ["from langchain.evaluation.qa import QAEvalChain"]}, {"cell_type": "code", "execution_count": 12, "id": "1612dec1", "metadata": {}, "outputs": [], "source": ["llm = OpenAI(temperature=0)\n", "eval_chain = QAEvalChain.from_llm(llm)\n", "graded_outputs = eval_chain.evaluate(dataset, predictions, question_key=\"question\", prediction_key=\"result\")"]}, {"cell_type": "markdown", "id": "79587806", "metadata": {}, "source": ["\u6211\u4eec\u53ef\u4ee5\u5c06\u5206\u7ea7\u8f93\u51fa\u6dfb\u52a0\u5230\u201c\u9884\u6d4b\u201d\u5b57\u5178\u4e2d\uff0c\u7136\u540e\u8ba1\u7b97\u5206\u7ea7\u7684\u6570\u91cf\u3002"]}, {"cell_type": "code", "execution_count": 13, "id": "2a689df5", "metadata": {}, "outputs": [], "source": ["for i, prediction in enumerate(predictions):\n", "    prediction['grade'] = graded_outputs[i]['text']"]}, {"cell_type": "code", "execution_count": 14, "id": "27b61215", "metadata": {}, "outputs": [{"data": {"text/plain": ["Counter({' CORRECT': 12, ' INCORRECT': 10})"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": ["from collections import Counter\n", "Counter([pred['grade'] for pred in predictions])"]}, {"cell_type": "markdown", "id": "12fe30f4", "metadata": {}, "source": ["\u6211\u4eec\u8fd8\u53ef\u4ee5\u5c06\u9519\u8bef\u793a\u4f8b\u7684\u6570\u636e\u70b9\u8fc7\u6ee4\u51fa\u6765\u5e76\u67e5\u770b\u5b83\u4eec\u3002"]}, {"cell_type": "code", "execution_count": 15, "id": "47c692a1", "metadata": {}, "outputs": [], "source": ["incorrect = [pred for pred in predictions if pred['grade'] == \" INCORRECT\"]"]}, {"cell_type": "code", "execution_count": 16, "id": "0ef976c1", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'question': 'What did the author write their dissertation on?',\n", " 'answer': 'The author wrote their dissertation on applications of continuations.',\n", " 'result': ' The author does not mention what their dissertation was on, so it is not known.',\n", " 'grade': ' INCORRECT'}"]}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": ["incorrect[0]"]}, {"cell_type": "code", "execution_count": null, "id": "7710401a", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.1"}}, "nbformat": 4, "nbformat_minor": 5}